# Learnings

- ComfyUI's attention functions are wrapped with `optimized_attention_override` via `transformer_options`, which lets a custom node swap attention backends without patching core code.
- Flux attention calls `optimized_attention(..., skip_reshape=True)` with `q/k/v` shaped `[B, H, N, D]` after RoPE, so backend overrides should handle that layout.
- Taylor attention now estimates its own activation memory and calls ComfyUI's `model_management.free_memory` to prompt offloading before large allocations.
- Early-probe and fp32 denominator options help avoid slow Taylor fallbacks when denominators go unstable.
- Sub-head block Taylor splits each head into smaller blocks to reduce feature dimension while keeping P fixed.
- Defaults now target diffusion-scale workloads (low min_tokens, sub-head blocks, and tighter block sizes).
- Taylor now aggregates denominator/quality stats per sampling step and logs a single summary per step.
- Added qk_normalize and scale_mul knobs to stabilize P=4 Taylor attention by shrinking q·k values.
- Added qk_norm_clip and qk_norm_power to stabilize P=4 without fully normalizing Q/K.
- Denominator fallbacks can now be gated by a fraction threshold to avoid over-triggering.
- Quality stats are now computed against unmodified attention, even when Q/K are adjusted.
- Auto-tune mode can search q/k scaling during early steps and lock in a best config.
- Step logs now include q/k norm and sampled q·k percentile diagnostics to measure regime mismatch.
- Added a Triton fused-kernel path that streams Taylor feature chunks to avoid full feature tensor allocation.
- Early-probe fallbacks now respect denom_fallback_frac_limit and log probe stats for debugging.
- Step stats now surface fallback reasons and max_head_dim for easier triage.
- Added fused value-dimension chunking and bf16 S-chunk storage to reduce memory in fused path.
- Step stats now log both quality_raw (unmodified) and quality_eff (modified) metrics.
- Feature-dimension fallback logs now always include the rejected R value.
- max_feature_dim_R UI limit raised to allow very large feature dimensions when experimenting with higher P.
- For P>=5, the fused path streams multiset feature indices on GPU to avoid Python tuple construction overhead.
- Added qk_norm_sigma_max to gate q/k normalization by sigma.
- Streaming fused memory reservation now estimates based on chunk sizes to avoid over-offloading.
- Added taylor_sigma_max and taylor_layer_start/end gates to skip Taylor at high sigma or outside block ranges.
- quality_raw and quality_eff now share the same sampled query indices for apples-to-apples comparisons.
- Added a fully fused Triton path (fused_full_kernel) that uses precomputed feature tables to avoid Python feature loops.
- Added a hybrid local/global attention node that patches Flux attention to combine local RoPE attention with a global low-dim Taylor approximation.
- Hybrid attention now patches both `flux.math` and `flux.layers` attention bindings via model pre-run/cleanup callbacks.
- Model-level callbacks must be registered on `ModelPatcher` (transformer_options callbacks are not invoked by pre-run/cleanup).
- Hybrid PCA now falls back to an identity projection when too few samples are available to fit a low-rank basis.
- Hybrid attention can optionally aggregate hybrid-vs-exact quality stats across steps and log once at cleanup.
- PCA projection cache now keys by dtype to avoid bf16/float32 matmul mismatches when force_fp32 is disabled.
- Hybrid local window can be scheduled by sigma (min/max + sigma low/high) to use full attention early and windowed attention later.
- Hybrid global weight ramp now decreases with sigma (full at low sigma, zero at high sigma) so approximations can be enabled later in sampling.
- Local window scheduling treats a value of 0 as "full attention" and substitutes the current sequence length during interpolation.
- Local window scheduling now uses a smoothstep (sigmoid-like) curve for gentler start/end transitions.
- Hybrid global Taylor now aligns accumulation dtype with force_fp32 to avoid bf16/fp32 einsum mismatches.
- Hybrid quality stats logs now include the active hybrid config parameters for reproducibility.
- Hybrid quality stats now append to `output/hybrid-attention-results.jsonl` including config and inferred meta (sigma, shapes, latent resolution where possible).
- Added ClockedSweepValues node for distributing test values evenly across a clock list.
- ClockedSweepValues accepts a single integer string to generate a 1..N clock and can infer clock length from values if left blank.
- Added Combinations node to generate repeated value lists covering all combinations across inputs.
- ClockedSweepValues and Combinations outputs are marked as float list outputs so downstream nodes receive expanded list values.
- Flux2TTR patches Flux attention via pre-run/cleanup callbacks and resolves per-layer TTR modules using `transformer_options["block_type"]` and `block_index`, so single-block replacements can stay layer-specific without editing ComfyUI core.
- ComfyUI node execution may run under `torch.inference_mode()`, so any in-node distillation/training must explicitly use `torch.inference_mode(False)` and `torch.enable_grad()` around backward passes.
- Flux2TTR inference speed improved substantially by replacing per-token Python scan loops with chunked vectorized prefix updates (`cumsum` over `k⊗v`), and by running CUDA inference in input dtype (bf16/fp16) while keeping training in fp32.
- Flux2TTR quality collapsed when trained on synthetic calibration tensors; distillation must use native Flux attention outputs from real attention calls as teacher targets during sampling.
- PyTorch blocks autograd when `nn.Linear` receives inference-mode tensors; for ComfyUI online distillation we must clone q/k/v (and teacher targets) inside `torch.inference_mode(False)` before backward.
- Flux2TTR now performs Taylor-style VRAM reservation (`model_management.free_memory`) before training/inference attention calls, reserving `1.1x` of estimated working memory so ComfyUI can offload earlier node allocations.
- `model_management.free_memory` is not a persistent reservation token; repeated-run VRAM growth was caused by retained runtime references, so cleanup must release runtime GPU tensors and unregister runtime IDs.
- Since cleanup unregisters runtime IDs, cached node outputs may reference missing runtimes on later prompts; runtime recovery from config/checkpoint prevents accidental native-attention fallback in that case.
- Flux2TTR training is much more memory-sensitive than inference; capping training chunk size (64) and retrying scan chunks on OOM prevents giant `kv_assoc.cumsum` allocations from immediately exhausting VRAM.
- For practical Flux VRAM limits, online distillation must use bounded token sampling and graceful OOM fallback (disable training but keep teacher passthrough) to avoid aborting the whole sampler run.
- Adaptive scan chunking should be sticky per layer after an OOM retry; otherwise each attention call re-attempts known-failing chunk sizes and wastes time.
- Distillation progress logs every fixed update interval (e.g., every 10 updates) are useful because Flux2TTR training steps are attention-call based, not sampler-step based.
- Flux2TTR online distillation often needs hundreds of updates for usable loss; setting the node default to 512 steps is a better starting point than 32.
- Training-mode preview is useful for qualitative monitoring, but it adds another student forward pass; keep it optional so distillation-only runs can stay in teacher passthrough for speed.
- Distillation quality is better tracked with multiple per-layer metrics (NMSE, cosine similarity, norm/mean/std ratios, p95/p99 error tails, sampled attention KL/top-k overlap) and streamed to Comet each update for long-run monitoring.
