# Learnings

- ComfyUI's attention functions are wrapped with `optimized_attention_override` via `transformer_options`, which lets a custom node swap attention backends without patching core code.
- Flux attention calls `optimized_attention(..., skip_reshape=True)` with `q/k/v` shaped `[B, H, N, D]` after RoPE, so backend overrides should handle that layout.
- Taylor attention now estimates its own activation memory and calls ComfyUI's `model_management.free_memory` to prompt offloading before large allocations.
- Early-probe and fp32 denominator options help avoid slow Taylor fallbacks when denominators go unstable.
- Sub-head block Taylor splits each head into smaller blocks to reduce feature dimension while keeping P fixed.
- Defaults now target diffusion-scale workloads (low min_tokens, sub-head blocks, and tighter block sizes).
- Taylor now aggregates denominator/quality stats per sampling step and logs a single summary per step.
- Added qk_normalize and scale_mul knobs to stabilize P=4 Taylor attention by shrinking q·k values.
- Added qk_norm_clip and qk_norm_power to stabilize P=4 without fully normalizing Q/K.
- Denominator fallbacks can now be gated by a fraction threshold to avoid over-triggering.
- Quality stats are now computed against unmodified attention, even when Q/K are adjusted.
- Auto-tune mode can search q/k scaling during early steps and lock in a best config.
- Step logs now include q/k norm and sampled q·k percentile diagnostics to measure regime mismatch.
- Added a Triton fused-kernel path that streams Taylor feature chunks to avoid full feature tensor allocation.
- Early-probe fallbacks now respect denom_fallback_frac_limit and log probe stats for debugging.
- Step stats now surface fallback reasons and max_head_dim for easier triage.
- Added fused value-dimension chunking and bf16 S-chunk storage to reduce memory in fused path.
- Step stats now log both quality_raw (unmodified) and quality_eff (modified) metrics.
- Feature-dimension fallback logs now always include the rejected R value.
- max_feature_dim_R UI limit raised to allow very large feature dimensions when experimenting with higher P.
- For P>=5, the fused path streams multiset feature indices on GPU to avoid Python tuple construction overhead.
- Added qk_norm_sigma_max to gate q/k normalization by sigma.
- Streaming fused memory reservation now estimates based on chunk sizes to avoid over-offloading.
- Added taylor_sigma_max and taylor_layer_start/end gates to skip Taylor at high sigma or outside block ranges.
- quality_raw and quality_eff now share the same sampled query indices for apples-to-apples comparisons.
- Added a fully fused Triton path (fused_full_kernel) that uses precomputed feature tables to avoid Python feature loops.
- Added a hybrid local/global attention node that patches Flux attention to combine local RoPE attention with a global low-dim Taylor approximation.
- Hybrid attention now patches both `flux.math` and `flux.layers` attention bindings via model pre-run/cleanup callbacks.
- Model-level callbacks must be registered on `ModelPatcher` (transformer_options callbacks are not invoked by pre-run/cleanup).
- Hybrid PCA now falls back to an identity projection when too few samples are available to fit a low-rank basis.
- Hybrid attention can optionally aggregate hybrid-vs-exact quality stats across steps and log once at cleanup.
- PCA projection cache now keys by dtype to avoid bf16/float32 matmul mismatches when force_fp32 is disabled.
- Hybrid local window can be scheduled by sigma (min/max + sigma low/high) to use full attention early and windowed attention later.
